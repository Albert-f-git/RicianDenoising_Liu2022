import numpy as np
from src.operators import forward_gradient, backward_divergence

def adjoint_test(shape=(100, 100)):
    # 1. ç”Ÿæˆéšæœºè¾“å…¥
    u = np.random.randn(*shape)
    # å‘é‡åœº p = (px, py)
    px = np.random.randn(*shape)
    py = np.random.randn(*shape)

    # 2. è®¡ç®—å·¦å¼: <grad u, p>
    # å…ˆè®¡ç®—æ¢¯åº¦
    grad_ux, grad_uy = forward_gradient(u)
    # è®¡ç®—å†…ç§¯ (ä¸¤ä¸ªåˆ†é‡åˆ†åˆ«ç›¸ä¹˜å†æ±‚å’Œ)
    lhs = np.sum(grad_ux * px + grad_uy * py)

    # 3. è®¡ç®—å³å¼: <u, -div p>
    # è®¡ç®—æ•£åº¦
    div_p = backward_divergence(px, py)
    # æ³¨æ„è¿™é‡Œå…¬å¼é‡Œé€šå¸¸å¸¦ä¸ªè´Ÿå·ï¼Œå–å†³äºä½ å¯¹ div çš„å®šä¹‰
    rhs = np.sum(u * (-div_p))

    # 4. æ¯”è¾ƒç»“æœ
    diff = np.abs(lhs - rhs)
    print(f"LHS: {lhs:.10f}")
    print(f"RHS: {rhs:.10f}")
    print(f"Difference: {diff:.10e}")
    
    return diff < 1e-10

if __name__ == "__main__":
    if adjoint_test():
        print("Adjoint test PASSED! ğŸš€")
    else:
        print("Adjoint test FAILED. âŒ è¯·æ£€æŸ¥æ•£åº¦çš„è¾¹ç•Œå¤„ç†ã€‚")